{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1phchZdq1njMnIDZEj_XaqgQ2UR_m_EG2","timestamp":1722951064538}],"authorship_tag":"ABX9TyNJQFNBR5pNKb90mv70qiSU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Problem Set 3 - Logistic Regression  \n","\n","In this problem set, you will train a logistic regression model to predict the extent of Greenland's ice slabs using the dataset of remote sensing and climate model variables that you explored in Problem Set 2.   \n","\n","### Data Set Variables:    \n","**X** - x coordinate of data point (EPSG 3413)       \n","**Y** - y coordinate of data point (EPSG 3413)         \n","**IceSlab** - binary variable indicating if an ice slab was observed at a given location, 0 = no ice slab, 1 = ice slab observed            \n","**HV** - horizontal-vertical polarized backscatter from the Sentinel-1 SAR satellite in digital number space        \n","**SAR_melt** - non-dimensional estimate of firn/snow summer water content from Sentinel-1 SAR satellite, lower values indicate more stored melt           \n","**MAR** - decadal average melt to accumulation (snowfall) ratio from a regional climate model         \n","**RACMO_melt** - decadal average surface melt from the regional climate model RACMO in mm of water equivalent per year           \n","**RACMO_snow** - decadal average snowfall rate from the regional climate model RACMO in mm of water equivalent per year          \n","**MAT** - mean annual temperature in degrees Celcius from the regional climate model MAR           \n","**Xpol** - cross polarized backscater ratio from the Sentinel-1 SAR satellite in digital number space             \n","**Elevation** - surface elevation in meters      "],"metadata":{"id":"6KVYfC7Bxiz_"}},{"cell_type":"markdown","source":["(1) In the code block below, import your packages."],"metadata":{"id":"PJp9Xxgtx4ap"}},{"cell_type":"code","source":[],"metadata":{"id":"H7DcToGn436U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(2) Load the from the course github at https://raw.githubusercontent.com/rtculberg/ml_in_eas/main/IceSlabs.csv. Display the first few rows of the dataframe."],"metadata":{"id":"8xxHq-rIyC38"}},{"cell_type":"code","source":[],"metadata":{"id":"lTnZcZmp44ZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(3) Check for and remove any rows that have NaN values. Remove any columns with variables that will not be used for training. Make a copy of the dataframe withou the labels. Normalize the data in your new data frame using a z-score transform."],"metadata":{"id":"ZnM0i4MbyGKr"}},{"cell_type":"code","source":[],"metadata":{"id":"b2oJ9Lvz457y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(4) Apply a PCA and determine the principal components (PCs) using the sklearn PCA fit_transform function (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Leave the n_components argument blank. This will default to keeping all components."],"metadata":{"id":"bDeDZ1fzyWTr"}},{"cell_type":"code","source":[],"metadata":{"id":"NEZ__yRmYuAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(5) Make a new variable called `x` that contains the first N principal components, where N is the number of principal components needed to explain at least 90% of the variance in the data set. Make a new variable called `y` that contains the labels."],"metadata":{"id":"WvTtBx4Uydb9"}},{"cell_type":"code","source":[],"metadata":{"id":"IpFo-IaZYzHs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(6) Check the count of data points where ice slabs were observed vs. the count of data points where no ice slabs were observed. Do we have a problem with class imbalance in this dataset?   \n","\n","**Hint:** check the pandas documentation for the `DataFrame.value_counts()` property."],"metadata":{"id":"VjqNNE5yyqoz"}},{"cell_type":"code","source":[],"metadata":{"id":"ASMsJbC2Y76U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(7) Split your dataset into a training set and test set. Use 80% of the data for training and 20% for testing."],"metadata":{"id":"NxJlarMLzC21"}},{"cell_type":"code","source":[],"metadata":{"id":"xqb9HC5tZDmd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(8) Instantiate a new logistic regression model that accounts for the class imbalance in the data set. Train your model on the training dataset."],"metadata":{"id":"IcaS2BtOzKtz"}},{"cell_type":"code","source":[],"metadata":{"id":"sgwMgwzlZHeN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(9) Since we have a significant class imbalance, the typical threshold of 0.5 for ice slab detection may not be the optimal detection threshold, even with balanced class weighting. Use the `metrics.precision_recall_curve()` function to get the precision and recall for a range of thresholds on the training set. Find the threshold that leads to the optimal F1 score. Plot the precision-recall curve and add a dot for the optimal detection point."],"metadata":{"id":"MLLEhoqlzT9r"}},{"cell_type":"code","source":[],"metadata":{"id":"RzYdJcb9ZKEd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(10) Using your optimized threshold, make predictions on the test set. Calculate the F1 score on the test set. Do we have a problem with overfitting? Why or why not?"],"metadata":{"id":"oBf3ADR90KOD"}},{"cell_type":"code","source":[],"metadata":{"id":"mAMNnivJZM6W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(11) Calculate and plot the confusion matrix for your results on the test set."],"metadata":{"id":"5j_43TRw0Xp9"}},{"cell_type":"code","source":[],"metadata":{"id":"3RCriBM3ZSXD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(12) Calculate the predicted probabilities on the full dataset. Add the probabilities as a new column to your dataframe of raw variables. Calculate the correlation matrix for the updated dataframe. Which of the original raw variables are most highly correlated with the predicted presence of ice slabs?"],"metadata":{"id":"r6NuPfNR0gwj"}},{"cell_type":"code","source":[],"metadata":{"id":"R0eTzX-wZbGe"},"execution_count":null,"outputs":[]}]}