{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1F2MFT220uVdGdEHRJg2ZTu3Re2Rc8uD7","timestamp":1722951627082}],"authorship_tag":"ABX9TyNIAlDWWUjqKGBH2HO1QEWa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lab 1 - Overfitting & Bias-Variance Tradeoff"],"metadata":{"id":"y_J0-FMWi09T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"16BwPi_iCSHj"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import linear_model, model_selection\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"markdown","source":["Run the code block below to generate a toy dataset that we will use to better understand the importance of training data size and what happens when we overfit our data."],"metadata":{"id":"WiOrKgWRgT4o"}},{"cell_type":"code","source":["# Generate a noisy dataset\n","x = np.arange(0,10,0.1)\n","y = 3*x + 1 + 5*np.random.randn(x.shape[0])"],"metadata":{"id":"PbxeNCX1Cs7x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Does the Size of the Training Data Set Matter?\n","\n","**Coding Work:**     \n","(1) Use the `test_train_split` function to randomly select 2% of the data to use for training the model.        \n","(2) Fit a linear model to the training data. On one plot, plot the training data set and the model predictions on the training set. On a second plot, plot the test set and the model predictions on the test set.     \n","(3) Print out the mean square error of the predictions on the training set and on the test set.   \n","\n","**Discuss the following questions with your lab group:**   \n","How does the mean square error compare between the training and test sets?    \n","How does this relationship change if instead of using 2% of the data for training, you use 5% of the data? What about if you use 20% of the data? How about 70% of the data?\n","What does this tell you about the importance of training data set size and representativeness?"],"metadata":{"id":"TF9L-9jvc3Ir"}},{"cell_type":"code","source":[],"metadata":{"id":"y7_h_19WEdfj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Complexity & Overfitting    \n","\n","**Code Work:**        \n","(1) Use the `train_test_split` funtion to randomly select 10% of the data for model training. Make sure to set the random state to some integer for reproducibility.     \n","(2) Use the code shell below to fit a second order polynomial model to the training data and plot the results as in Part 1.      \n","(3) Print out the mean square error of the predictions on the training set and on the test set.   \n","\n","**Discuss the following questions with your lab group:**   \n","Try fitting polynomials of degree 2, 4, 6, 8, and 10 to training data.        \n","How does relationship between the mean square error on training data set and the mean square error on the test data set change as you increase the polynomial degree?       \n","Do you think that any of these polynomial models are overfit? Which ones and why?  \n","How would your answer change if you trained the model on 70% of the data instead of 10%?"],"metadata":{"id":"NwGMSvwAgoHQ"}},{"cell_type":"code","source":["# Enter your code for the test-train split here\n","\n","\n","poly_degree = 2   # update the degree of the polynomial to fit\n","\n","# Generate the polynomial features for the train and tes set\n","poly = PolynomialFeatures(degree=poly_degree, include_bias=False)\n","poly_train = poly.fit_transform(x_train)\n","poly_test = poly.fit_transform(x_test)\n","\n","# Train the linear regression model with the polynomial features\n","poly_model = linear_model.LinearRegression()\n","poly_model.fit(poly_train, y_train)\n","y_pred_poly = poly_model.predict(poly_test)\n","y_pred_poly_train = poly_model.predict(poly_train)\n","\n","sorted = np.argsort(x_test, axis=0)\n","\n","text_kwargs = dict(ha='left', va='center', fontsize=12)\n","\n","plt.close(\"all\")\n","fig, (ax1, ax2) = plt.subplots(1,2, layout=\"constrained\")\n","ax1.scatter(x_train, y_train, s=10)\n","ax1.plot(x_test[sorted[:,0]], y_pred_poly[sorted[:,0]], color=\"orange\")\n","ax1.text(0, -14, \"MSE: %.2f\" % mean_squared_error(y_train, y_pred_poly_train), **text_kwargs)\n","ax2.scatter(x_test, y_test, s=10)\n","ax2.plot(x_test[sorted[:,0]], y_pred_poly[sorted[:,0]], color=\"orange\")\n","ax2.text(0, -14, \"MSE: %.2f\" % mean_squared_error(y_test, y_pred_poly), **text_kwargs)\n","ax1.set_xlabel(\"X\")\n","ax1.set_ylabel(\"Y\")\n","ax1.set_title(\"Training Data\")\n","ax2.set_xlabel(\"X\")\n","ax2.set_ylabel(\"Y\")\n","ax2.set_title(\"Test Data\")\n","fig.set_size_inches(10,4, forward=True)\n","plt.show()"],"metadata":{"id":"nYltf8sIGOCc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bias-Variance Tradeoff"],"metadata":{"id":"zkLLzfkbbTyA"}},{"cell_type":"markdown","source":["Bias is error that comes from eroneous assumptions in our machine learning algorithm - for example, trying to fit data generated from a cubic polynomial with a simple linear model.     \n","Variance is error from sensitivity to small fluctuations in the training set. High variance could be the result of a training data set that is too small, or from overfitting where the algorithm is modeling random noise in the training data set.        \n","In general, there is a trade-off between bias and variance. More complex model will have lower bias, since they can capture more features of the relationship between inputs and outputs. However, they can also have higher variance due to overfitting. An optimal model tries to jointly minimize bias and variance. In the code below, we will explore this bias-variance tradeoff.   \n","\n","Run the first code block to generate a new toy dataset."],"metadata":{"id":"t6uGkvvNkBcg"}},{"cell_type":"code","source":["# Generate a noisy dataset\n","x = np.arange(-10,10,0.2)\n","y = 0.01*(x**3 + 1 + 280*np.random.randn(x.shape[0]))"],"metadata":{"id":"M6tmUcH9UbjC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code below comes bias and variance between a simple linear model and a polynomial model of order `poly_degree`. The script runs 20 iterations in which it randomly selects a training dataset of size `1-sample_size` and fits the two models. The \"Bias\" plot shows a histogram of the mean square error of the models on the 20 training sets. A low value means that the model is effectively capturing most of the variation present in the training data set. The \"Variance\" plot shows the 20 linear and 20 polynomial model predictions over the training set. A high spread vertical spread across the 20 models indicates high variance - the model coefficients change a lot depending on the data used to train it.     \n","\n","**Run the script with `sample_size = 0.8` and for `poly_degree` equal to 3,5,7, 9, and 11. Then discuss the following questions with your lab group:**     \n","How does the bias of the higher order polynomial model change as you increase the polynomial degree?         \n","How does the variance of the polynomial models change as you increase the polynomial degree?        \n","How does the higher order polynomial bias and variance compare to the linear model?            \n","What seems like a reasonable polynomial degree for modeling this data set? Why?"],"metadata":{"id":"L9dZ09uVnbDf"}},{"cell_type":"code","source":["# Bias-Variance Tradeoff\n","\n","poly_degree = 11      # polynomial model degree\n","sample_size = 0.8     # size of the test set (training set = 1 - sample_size)\n","\n","Niter = 20            # Number of iterations to run\n","# Variables to save the results from each iterations model fitting\n","length = np.rint((1-sample_size)*x.shape[0]).astype(int)\n","x_lin = np.empty((length,Niter))\n","y_pred_lin = np.empty((length,Niter))\n","bias_lin = np.empty(Niter)\n","x_poly = np.empty((length,Niter))\n","y_pred_poly = np.empty((length,Niter))\n","bias_poly = np.empty(Niter)\n","\n","# Fit linear and polynomial models on 20 randomly selected training data sets\n","for k in range(0,Niter):\n","  x_train, x_test, y_train, y_test = model_selection.train_test_split(x.reshape(-1,1), y.reshape(-1,1), test_size=sample_size)\n","  # Linear Regression\n","  regr = linear_model.LinearRegression()\n","  regr.fit(x_train, y_train)\n","  x_lin[:,k] = x_train[:,0]\n","  tmp = regr.predict(x_train)\n","  y_pred_lin[:,k] = tmp[:,0]\n","  bias_lin[k] = mean_squared_error(y_train, tmp)\n","  # Polynomial Regression\n","  poly = PolynomialFeatures(degree=poly_degree, include_bias=False)\n","  poly_train = poly.fit_transform(x_train)\n","  poly_model = linear_model.LinearRegression()\n","  poly_model.fit(poly_train, y_train)\n","  sorted = np.argsort(x_train, axis=0)\n","  tmp = poly_model.predict(poly_train)\n","  y_pred_poly[:,k] = tmp[sorted[:,0]][:,0]\n","  x_poly[:,k] = x_train[sorted[:,0]][:,0]\n","  bias_poly[k] = mean_squared_error(y_train, tmp)\n","\n","# Plot the results\n","plt.close(\"all\")\n","fig, (ax1, ax2) = plt.subplots(1,2, layout='constrained')\n","\n","ax1.hist(bias_lin, facecolor='blue',alpha=0.5,edgecolor='black', density=True, label=\"Linear Model\")\n","ax1.hist(bias_poly, facecolor='orange',alpha=0.5,edgecolor='black', density=True, label=\"Higher Order Polynomial\")\n","ax1.set_xlabel(\"Mean Square Error\")\n","ax1.set_ylabel(\"Probability Density\")\n","ax1.set_title(\"Bias\")\n","ax1.legend()\n","\n","ax2.scatter(x,y,s=5, color='black')\n","for k in range(0,Niter):\n","  ax2.plot(x_lin[:,k], y_pred_lin[:,k], color='blue', alpha=0.5)\n","for k in range(0,Niter):\n","  ax2.plot(x_poly[:,k], y_pred_poly[:,k], color='orange', alpha=0.5)\n","ax2.set_xlabel(\"X\")\n","ax2.set_ylabel(\"Y\")\n","ax2.set_title(\"Variance\")\n","\n","fig.set_size_inches(10,4, forward=True)\n","plt.show()"],"metadata":{"id":"C3vqXLTcPgqx"},"execution_count":null,"outputs":[]}]}