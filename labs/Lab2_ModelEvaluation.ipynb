{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86n0N69BfZ3c"
   },
   "source": [
    "# Lab 2 - Logistic Regression and Model Evaluation      \n",
    "\n",
    "The purpose of this lab is to:      \n",
    "(1) Familiarize you with the implementing logistic regression in Scikit Learn.     \n",
    "(2) Explore different ways to evaluate binary classification models and practice interpreting those evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07_yFvvWohxF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import sklearn as sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr4bGOWtmmEC"
   },
   "source": [
    "## Logistic Regression        \n",
    "\n",
    "In this lab, you will use the Von Mises stress to predic the presence of absence of fractures on the surface of the Greenland Ice Sheet. The code block below loads your lab data set and drops any columns with missing data. You will see the following variables in the data set:     \n",
    "\n",
    "**label** - 0 if no crevasses were detected at that location, 1 if crevasses were detected at that location         \n",
    "**stress** - the Von Mises stress in Pascals (Pa) at the given location     \n",
    "\n",
    "Create new variables called `means_stress` and `std_stress` and save the mean of the stress column and the standard deviation of the stress column in the appropriate variable. Then apply a z-score transform to the stress column. Print the first few lines of the dataframe to verify that the transform was applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuenSr5vpWGz"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/rtculberg/ml_in_eas/main/data/CrevasseData2.csv\")\n",
    "data = data.dropna(axis=0, how='any')\n",
    "\n",
    "# Add your code here to record the mean and standard deviation of the stress data, then apply the z-score\n",
    "# transform to the stress column in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTr1ooHtg0HE"
   },
   "source": [
    "Run the code block below to plot histograms of the Von Mises stress in crevasses and uncrevassed areas. With your lab group, discuss the following questions:            \n",
    "\n",
    "How separable do these two categories appear to be? Is there are clear cutoff in stress between crevassed and uncrevassed regions?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHnYbYgKuQ3G"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.hist(data['stress'][data['label'] == 0], label=\"uncrevassed\", alpha = 0.5)\n",
    "ax.hist(data['stress'][data['label'] == 1], label=\"crevassed\", alpha = 0.5)\n",
    "ax.set_xlabel('Normalized Von Mises Stress'); plt.ylabel('Count')\n",
    "ax.set_title('Data Distributions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gwY2Lrdhd73"
   },
   "source": [
    "Check for a class imbalance. How does our sample size for crevassed vs. uncrevassed areas compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP8zOaumszjL"
   },
   "outputs": [],
   "source": [
    "# check for class imbalance\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32XHdpiXhnqo"
   },
   "source": [
    "The code block below splits your dataset in the train, validate, and test datasets. We will use 70% of data for training, 10% for validation, and 20% for testing. We need a separate validation dataset because we eventually want to tune the detection threshold for crevasses.      \n",
    "Note that the `LogisticRegression` object takes as input 2D arrays of shape (X,1) where X is the number of data samples. Therefore, we need to use the `.reshape(-1,1)` function to get our data from a 1D array into a 2D array (e.g. shape goes from (X,) to (X,1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rGxu5eIs8vf"
   },
   "outputs": [],
   "source": [
    "# split training and test data\n",
    "x_train,x_test,y_train,y_test=train_test_split(data['stress'],data['label'],test_size=0.2,random_state=10)\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.125,random_state=10)\n",
    "x_train = x_train.to_numpy().reshape(-1,1)\n",
    "y_train = y_train.to_numpy().reshape(-1,1)\n",
    "x_val = x_val.to_numpy().reshape(-1,1)\n",
    "y_val = y_val.to_numpy().reshape(-1,1)\n",
    "x_test = x_test.to_numpy().reshape(-1,1)\n",
    "y_test = y_test.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uQ1uFhEiUL6"
   },
   "source": [
    "Add code below instantiate a new `LogisticRegression` object called `regress`. Remember to use `class_weight='balanced'` to account for any class imbalances in the dataset. Then fit the logistic regression model on the training dataset. You may find the documentation for `LogisticRegression` and the associated examples helpful: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vySvR8nftFb8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b56MYTk-jPbM"
   },
   "source": [
    "In the code block below, we would like to plot the model-predicted probability of fracture as a function of the Von Mises stress. To do so, we can generate an array of evenly spaced stress values between and feed it to the `predict_proba()` function to predict the probability of fracture for each stress value.      \n",
    "\n",
    "At the top of the code block, generate an array of values from -3 to 5 in increments of 0.05 and call the new variables `stress`. Then use the `predict_proba()` attribute of your `LogisticRegression` object to predict the probability of fracture for each values in this new array. Call this new variable `prob`.     \n",
    "\n",
    "You should see that `prob` is an (X,2) array (where X is the size of the trainign dataset). This is because `LogisticRegression` provides predictions for both class labels. So `prob[:,0]` is the probability of no fracture for each stress value and `prob[:,1]` is the probability of fracture for each stress value.     \n",
    "\n",
    "By running the completed code block you should see a plot comparing the probability of fracture to the distributions of crevassed and uncrevassed points in the original data. The red dashed line marks the default threshold for discriminating between crevassed and uncrevassed areas. It assumes that if we predict that a location has a >=50% probability of fracture, it should be marked as crevassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvuvsL4ftJ9_"
   },
   "outputs": [],
   "source": [
    "# Add your code here to generate a new array of normalized stress values between\n",
    "# -3 and 5 and then predict the probability of fracture on this new array\n",
    "\n",
    "\n",
    "# Calculate the stress threshold of discriminating between fracture and no fracture\n",
    "# assuming a default cutoff probability of 50%\n",
    "threshold = np.argmin(np.abs(prob[:,1] - 0.5))\n",
    "\n",
    "# Plot the model predictions vs. the distribution of the raw data\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "im = ax.hist2d((data['stress']*std_stress + mean_stress)/1000, data['label'], (100,50))\n",
    "ax.plot((stress*std_stress + mean_stress)/1000, prob[:,1], color=\"white\", label=\"Logistic Regression Fit\")\n",
    "ax.vlines((stress[threshold]*std_stress + mean_stress)/1000,0,1, color=\"red\", linestyle=\"dashed\")\n",
    "ax.plot((stress*std_stress + mean_stress)/1000, 0.5*np.ones(stress.shape), color=\"white\", linestyle=\"dotted\")\n",
    "ax.set_xlabel('Von Mises Stress (kPa)'); plt.ylabel('Probability of Fracture')\n",
    "ax.set_title('Logistic Regression Model')\n",
    "cbar = fig.colorbar(im[3], ax=ax, label=\"Data Count\")\n",
    "plt.legend(loc=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIsKtLtfcz-l"
   },
   "source": [
    "## Evaluation Metrics   \n",
    "\n",
    "In this section of the lab, we will explore different ways to evaluate our model for predicting crevassed regions. In the code blocks below, complete the following tasks.\n",
    "    \n",
    "(1) Plot the confusion matrix for the test set. Set `normalize='true'` to list the classification rates, rather than the raw counts. See the documentation for `metrics.confusion_matrix` and `metrics.ConfusionMatrixDisplay` for help.      \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html                 \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzWxJTzyuDe1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F60Ma5DYmDf4"
   },
   "source": [
    "(2) List the recall, precision, f1 score, and Matthews Correlation Coefficent on the test set. See the documentation for `metrics.precision_recall_fscore_support` and `metrics.matthews_corrcoef`.    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html      \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23ku6ih8fZ7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVePWx2xmGm5"
   },
   "source": [
    "(3) Plot the ROC curve with the change line and list the area under curve (AUC) metric on the test set. See the documentation for `metrics.RocCurveDisplay`.         \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVZRhkbUZp64"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD865te1me0C"
   },
   "source": [
    "## Cross Fold Validation         \n",
    "\n",
    "As we discussed in class, one of the ways that we can avoid overfitting and ensure that our model evaluation metrics are fully representative of the data is to use cross-fold validation. In the code below, you will explore the difference between using standard stratified K-Fold validation and regionally stratified K-fold validation for our crevasse dataset.      \n",
    "\n",
    "The first code block applies standard K-fold validation. In this version, we randomly split our data into 10 different test and training datesets, retrain the model 10 times, and evaluate each model on a different test set. The use of `model_selection.StratifiedShuffleSplit` makes sure that each test and training dataset has the same proportion of crevassed and uncrevassed data as the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44LZwhItmhpC"
   },
   "outputs": [],
   "source": [
    "# Define the evaluation metrics that we want to run on each model iteration\n",
    "scoring = ['precision_weighted', 'recall_weighted', 'f1_weighted', 'matthews_corrcoef', 'roc_auc']\n",
    "# Define how our 10 folds, where the test set is 20% of the data and remaining 80% is for training\n",
    "cv = model_selection.StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "# Run the cross validation and get the resulting evaluation metrics\n",
    "scores = model_selection.cross_validate(regress, data['stress'].to_numpy().reshape(-1, 1), data['label'].to_numpy(), cv=cv, scoring=scoring)\n",
    "\n",
    "# Print the mean evaluation metrics, ply or minus 1 standard deviation\n",
    "print('Precision=%.3f+/-%.3f, Recall=%.3f+/-%.3f, F-Score=%.3f+/-%.3f, MCC=%.3f+/-%.3f, AUC=%.3f+/-%.3f' %\n",
    "      (np.mean(scores['test_precision_weighted']), np.std(scores['test_precision_weighted']),\n",
    "       np.mean(scores['test_recall_weighted']), np.std(scores['test_recall_weighted']),\n",
    "       np.mean(scores['test_f1_weighted']), np.std(scores['test_f1_weighted']),\n",
    "       np.mean(scores['test_matthews_corrcoef']), np.std(scores['test_matthews_corrcoef']),\n",
    "       np.mean(scores['test_roc_auc']), np.std(scores['test_roc_auc'])))\n",
    "\n",
    "# Plot the F1 score, MCC, and AUC metrics for each fold\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.plot(scores['test_f1_weighted'], '-o',label=\"F1 Score\")\n",
    "ax.plot(scores['test_matthews_corrcoef'], '-o',label=\"MCC\")\n",
    "ax.plot(scores['test_roc_auc'], '-o',label=\"AUC\")\n",
    "ax.set_xlabel('Fold Number'); plt.ylabel('Score')\n",
    "ax.set_title('10-Fold Cross Validation with Split Shuffle')\n",
    "ax.set_ylim((0,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJLWn4dqnvRb"
   },
   "source": [
    "The second code block applies regional cross-validation. In the provided dataset, the data is ordered by spatial distance from the northern most point in the dataset. So if we define our test and training datasets by position in the original dataset, we can break create test folds that from different regions of the ice sheet. The code below divides the original dataset into 10 consecutive, non-overlapping folds (e.g. fold 1 is indices 0-1000, fold 2 is indices 1001-2000, etc). For iteration of model training the corresponding fold serves as the test set, and the remaining data is used for training. By looking at how the model evaluation varies across these regional folds, we can see if our model performs better in some regions of the ice sheet than in others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pocqqfts5J99"
   },
   "outputs": [],
   "source": [
    "# Define the index range for each fold\n",
    "folds = 10\n",
    "test_len = np.rint(data['stress'].shape[0]/folds).astype(int)\n",
    "test_folds = 9*np.zeros(data['stress'].shape)\n",
    "for i in range(0,folds):\n",
    "  test_folds[i*test_len:(i+1)*test_len] = i\n",
    "\n",
    "# Create a predefined split object using these manually generate test fold indices\n",
    "ps = model_selection.PredefinedSplit(test_folds)\n",
    "\n",
    "# Define the evaluation metrics that we want to run on each model iteration\n",
    "scoring2 = ['precision_weighted', 'recall_weighted', 'f1_weighted', 'matthews_corrcoef', 'roc_auc']\n",
    "# Run the cross validation\n",
    "scores2 = model_selection.cross_validate(regress, data['stress'].to_numpy().reshape(-1, 1), data['label'].to_numpy(), cv=ps, scoring=scoring2, return_indices=True)\n",
    "\n",
    "# Print the mean evaluation metrics, ply or minus 1 standard deviation\n",
    "print('Precision=%.3f+/-%.3f, Recall=%.3f+/-%.3f, F-Score=%.3f+/-%.3f, MCC=%.3f+/-%.3f, AUC=%.3f+/-%.3f' %\n",
    "      (np.mean(scores2['test_precision_weighted']), np.std(scores2['test_precision_weighted']),\n",
    "       np.mean(scores2['test_recall_weighted']), np.std(scores2['test_recall_weighted']),\n",
    "       np.mean(scores2['test_f1_weighted']), np.std(scores2['test_f1_weighted']),\n",
    "       np.mean(scores2['test_matthews_corrcoef']), np.std(scores2['test_matthews_corrcoef']),\n",
    "       np.mean(scores2['test_roc_auc']), np.std(scores2['test_roc_auc'])))\n",
    "\n",
    "# Plot the F1 score, MCC, and AUC metrics for each fold\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.plot(scores2['test_f1_weighted'], '-o', label=\"F1 Score\")\n",
    "ax.plot(scores2['test_matthews_corrcoef'],'-o', label=\"MCC\")\n",
    "ax.plot(scores2['test_roc_auc'], '-o',label=\"AUC\")\n",
    "ax.set_xlabel('Fold Number'); plt.ylabel('Score')\n",
    "ax.set_title('10-Fold Cross Validation with Regional Folds')\n",
    "ax.set_ylim((0,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nVeNXlkpN8E"
   },
   "source": [
    "With your lab group, discuss the following questions:         \n",
    "\n",
    "(1) How do the average model evaluation metrics compare between the random stratified k-fold method and the regional k-fold method?    \n",
    "(2) If you ran only a a random k-fold validation and did not consider the impact of spatial correlation and regional variations in ice sheet properties, would you over or underestimate the true performance of your model?         \n",
    "(3) Looking at the regional k-fold method figure, which are there any spatial trends in model performance that you see?    \n",
    "(4) Looking at the regional k-fold method figure, you should see that folds 8&9 have high F1 scores but very low MCC scores. What might be an explanation for this difference in the evaluation metrics?           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoXz0rPsgUcu"
   },
   "source": [
    "## Hyperparameter Tuning   \n",
    "\n",
    "At the beginning of this lab, we assumed that the optimal probability cutoff of discriminatin between crevassed and uncrevassed areas was a probability of 50%. However, this is a hyperparmeter that we can tune to try and improve our model performance. There are a variety of ways to pick an \"optimal\" detection thresholds. Two of the most common are using the J statistic with the ROC curve or the F1 score with recall-precision curve.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mMysgvdq6yU"
   },
   "source": [
    "The code block below shows you how to use the J statistic to pick the optimal threshold for crevasse detection and plot the ROC curve with is point. Remember from lecture that the J statistic helps us find the point on the ROC curve that is furthest from chance level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o45iFCkfgykM"
   },
   "outputs": [],
   "source": [
    "# Use our trained model to predict the probabilty of fracture on our validation sset\n",
    "y_score = regress.predict_proba(x_val)\n",
    "\n",
    "# Calculate the false positive rate and true positive rate for a range of different\n",
    "# discrimination thresholds\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_score[:,1])\n",
    "# Calculate the J statistic and find it's maximum value\n",
    "J = tpr - fpr\n",
    "ind = np.argmax(J)\n",
    "\n",
    "# Plot the ROC curve\n",
    "metrics.RocCurveDisplay.from_estimator(regress,x_val,y_val, plot_chance_level=True)\n",
    "# Add a point showing where the J-statistic is maximized\n",
    "plt.scatter(fpr[ind], tpr[ind], color=\"orange\", label=\"Optimal Threshold - J Statistic\")\n",
    "plt.legend()\n",
    "\n",
    "# Print out the best probability threshold and the associated J statistic\n",
    "print('Best Threshold=%f, J Statistic=%.3f' % (thresholds[ind], J[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUmyCM2gq88s"
   },
   "source": [
    "The next code block demonstrates how we can use this optimized detection threshold to predict crevassed vs. uncrevassed areas on our test set. We can then calculate our new model evaluation metrics on this (hopefully) improved classification.    \n",
    "\n",
    "Run the code below, then discuss the following questions with your lab group:       \n",
    "(1) How did the evaluation metrics change once you applied the new detection threshold? (Compare these results to what you found in the original \"Evaluation Metrics\" section of this lab.)        \n",
    "(2) Is the updated model better at predicting the presence or absence of crevasses?   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZON6sZPiI1K"
   },
   "outputs": [],
   "source": [
    "# Predict the probability of fracture on the text dataset\n",
    "prob = regress.predict_proba(x_test)\n",
    "# Create a new empty matrix to hold crevassed vs. uncrevassed prediction\n",
    "y_pred = np.empty(prob.shape[0])\n",
    "# If the probabilty exceeds the newly defined threshold, set the prediction to 1\n",
    "y_pred[prob[:,1] >= thresholds[ind]] = 1\n",
    "y_pred[prob[:,1] < thresholds[ind]] = 0\n",
    "\n",
    "# Create and plot the foncusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the model evaluation metrics\n",
    "precision, recall, f1, support = metrics.precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "mcc = metrics.matthews_corrcoef(y_test, y_pred)\n",
    "print('Precision=%.3f, Recall=%.3f, F-Score=%.3f, MCC=%.3f' % (precision, recall, f1, mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux_licGnq-tD"
   },
   "source": [
    "In the code block below, now write your own code to use the f1 score to pick the optimal threshold for crevasse detection. Plot this optimal point on the recall-precision curve.        \n",
    "You will find the `metrics.precision_recall_curve` function very helpful: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html. Note that you will also need to zero out an NaN values in the F1 scores you compute before trying to find the maximum F1 score in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r52xo2lSioyN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK60Tc2prA1r"
   },
   "source": [
    "Classify the test dataset using this your newly optimized threshold, plot the new confusion matrix, and list the recall, precision, f1 score, and MCC.        \n",
    "\n",
    "Then discuss the following questions with your lab group:       \n",
    "(1) Are the optimal detection threshold or model evaluation results different when using the F1 score vs. the J statistic to select the detection threshold?     \n",
    "(2) Which method do you think is most appropriate for this particular use case (detecting crevasses)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAKxD-GfdX9X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3MVvUia8h2uInLYasPnCE",
   "provenance": [
    {
     "file_id": "1u1qhrOt8rv4i6NJqbKkdYIBspE5bzOy9",
     "timestamp": 1723485107441
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
