{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KVYfC7Bxiz_"
   },
   "source": [
    "# Problem Set 3 - Logistic Regression  \n",
    "\n",
    "In this problem set, you will train a logistic regression model to predict the extent of Greenland's ice slabs using the dataset of remote sensing and climate model variables that you explored in Problem Set 2.   \n",
    "\n",
    "### Data Set Variables:    \n",
    "**X** - x coordinate of data point (EPSG 3413)       \n",
    "**Y** - y coordinate of data point (EPSG 3413)         \n",
    "**IceSlab** - binary variable indicating if an ice slab was observed at a given location, 0 = no ice slab, 1 = ice slab observed            \n",
    "**HV** - horizontal-vertical polarized backscatter from the Sentinel-1 SAR satellite in digital number space        \n",
    "**SAR_melt** - non-dimensional estimate of firn/snow summer water content from Sentinel-1 SAR satellite, lower values indicate more stored melt           \n",
    "**MAR** - decadal average melt to accumulation (snowfall) ratio from a regional climate model         \n",
    "**RACMO_melt** - decadal average surface melt from the regional climate model RACMO in mm of water equivalent per year           \n",
    "**RACMO_snow** - decadal average snowfall rate from the regional climate model RACMO in mm of water equivalent per year          \n",
    "**MAT** - mean annual temperature in degrees Celcius from the regional climate model MAR           \n",
    "**Xpol** - cross polarized backscater ratio from the Sentinel-1 SAR satellite in digital number space             \n",
    "**Elevation** - surface elevation in meters      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJp9Xxgtx4ap"
   },
   "source": [
    "**[1]** In the code block below, import your packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7DcToGn436U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xxHq-rIyC38"
   },
   "source": [
    "**[2] (2 pts)** Load the from the course github at https://raw.githubusercontent.com/rtculberg/ml_in_eas/main/data/IceSlabs.csv. Display the first few rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTnZcZmp44ZL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnM0i4MbyGKr"
   },
   "source": [
    "**[3] (2 pts)** Check for and remove any rows that have NaN values. Make a copy of the dataframe and remove any columns with variables that will not be used for training, as well as the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2oJ9Lvz457y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4] (3 pts)** Split your data into a training, validation, and test sets. Use 70% of the data for training, 15% for validation, and 15% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5] (3 pts)** Apply a z-score transform to the input features. Don't forget to standarize the test and validation sets using the statistics from the training set! Print the first few rows of the training data set input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDeDZ1fzyWTr"
   },
   "source": [
    "**[6] (5 pts)** Apply PCA to the training features and determine the principal components (PCs) using the sklearn PCA fit() and transform() functions (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Leave the n_components argument blank. This will default to keeping all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEZ__yRmYuAT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvTtBx4Uydb9"
   },
   "source": [
    "**[7] (5 pts)** Make a new variable called `x` that contains the first N principal components, where N is the number of principal components needed to explain at least 90% of the variance in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpFo-IaZYzHs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjqNNE5yyqoz"
   },
   "source": [
    "**[8] (2 pts)** Check the count of data points where ice slabs were observed vs. the count of data points where no ice slabs were observed. Do we have a problem with class imbalance in this dataset?   \n",
    "\n",
    "**Hint:** check the pandas documentation for the `DataFrame.value_counts()` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASMsJbC2Y76U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcaS2BtOzKtz"
   },
   "source": [
    "**[9] (5 pts)** Instantiate a new logistic regression model that accounts for the class imbalance in the data set. Train your model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgwMgwzlZHeN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLLEhoqlzT9r"
   },
   "source": [
    "**[10] (7 pts)** Since we have a significant class imbalance, the typical threshold of 0.5 for ice slab detection may not be the optimal detection threshold, even with balanced class weighting. Use the `metrics.precision_recall_curve()` function to get the precision and recall for a range of thresholds on the validation set. Find the threshold that leads to the optimal F1 score. Plot the precision-recall curve and add a dot for the optimal detection point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzYdJcb9ZKEd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBf3ADR90KOD"
   },
   "source": [
    "**[11] (5 pts)** Using your optimized threshold, make predictions on the test set. Calculate the F1 score on the test set. Do we have a problem with overfitting? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAMNnivJZM6W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j_43TRw0Xp9"
   },
   "source": [
    "**[12] (5 pts)** Calculate and plot the confusion matrix for your results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RCriBM3ZSXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6NuPfNR0gwj"
   },
   "source": [
    "**[13] (6 pts)** Calculate the model prediction of ice slab probability on the full dataset (don't forget to apply scaling and PCA decomposition first!). Add the probabilities as a new column to your original dataframe of raw variables. Calculate the correlation matrix for the updated dataframe. Which of the original raw variables are most highly correlated with the predicted presence of ice slabs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0eTzX-wZbGe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrtnVK+OlVwbso1H5hfRLC",
   "provenance": [
    {
     "file_id": "1phchZdq1njMnIDZEj_XaqgQ2UR_m_EG2",
     "timestamp": 1722951064538
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
