{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10n2GKpTG-XMO_71i4aXv-FWYMRTBV7D8","timestamp":1724349943292}],"authorship_tag":"ABX9TyPwEPAnnNSMyNarnSO46Cbz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lab 4 - Clustering   \n","\n","The objective of this lab is to explore some of assumptions and limitations underlying the K-Means model and compare it's performance to a Gaussian Mixture Model for different types of cluster structures. For this purpose, we will use a toy data set, but for example, you could encounter similar issues when analyzing sediment provenance based on geochemical data."],"metadata":{"id":"_6gn9CooE_N-"}},{"cell_type":"markdown","source":["First, let's import the packages that we need to run our model."],"metadata":{"id":"EEAuSc7eFnNI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hh3tY-10WtAB"},"outputs":[],"source":["import numpy as np\n","from sklearn import preprocessing\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm"]},{"cell_type":"markdown","source":["The code block below generates four toy datasets with different cluster structures.     \n","\n","**Gaussian Blobs**: all three clusters have a bivariate gaussian structure with a single variance that is similar between blobs.        \n","**Anisotropically Distributed Blobs**: each cluster has a full covariance matrix (e.g. blobs are \"tilted\" and have different standard deviations for each variable, leading to an elliptical profile).         \n","**Unequal Variance Blobs**: each cluster has a different variance.       \n","**Unevenly Size Blobs**: each cluster contains a significantly different number of data points.  \n","\n","Run this code block to visualize the data distribution and true clusters for each dataset."],"metadata":{"id":"XNpZari1FrKn"}},{"cell_type":"code","source":["n_samples = 1500\n","random_state = 170\n","transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n","\n","# Gaussian blobs\n","X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n","# Anisotropically distributed blobs\n","X_aniso = np.dot(X, transformation)\n","# Unequal variance blobs\n","X_varied, y_varied = make_blobs( n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)\n","# Unevenly size blobs\n","X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n","y_filtered = [0] * 500 + [1] * 100 + [2] * 10\n","\n","# Plot all the blobs\n","fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n","axs[0, 0].scatter(X[:, 0], X[:, 1], c=y)\n","axs[0, 0].set_title(\"Gaussian Blobs with Similar Variance\")\n","axs[0, 1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y)\n","axs[0, 1].set_title(\"Anisotropically Distributed Blobs\")\n","axs[1, 0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied)\n","axs[1, 0].set_title(\"Unequal Variance Blobs\")\n","axs[1, 1].scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_filtered)\n","axs[1, 1].set_title(\"Unevenly Sized Blobs\")\n","plt.suptitle(\"Ground truth clusters\").set_y(0.95)\n","plt.show()"],"metadata":{"id":"hW86KOFYkvLx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## How Does K-Means Perform?    \n","\n","For each of the four datasets, instantiate a new `KMeans` object, train the model on the full dataset of X values, and predict the clusters (e.g. Y values). For each dataset, create an X vs. Y scatter plot of the data, where each data point is colored by the cluster that the K-Means algorithm predicts it should belong to.           \n","\n","Specific instructions for each dataset:    \n","**Gaussian Blobs**: set `n_clusters=2`, `n_init=auto`, `random_state=random_state`             \n","**Anisotropically Distributed Blobs**: set `n_clusters=3`, `n_init=auto`, `random_state=random_state`           \n","**Unequal Variance Blobs**: set `n_clusters=3`, `n_init=auto`, `random_state=random_state`      \n","**Unevenly Size Blobs**: set `n_clusters=3`, `n_init=auto`, `random_state=random_state`    \n","\n","Discuss with your lab group:      \n","(1) How well does the K-Means model appear to do in recovering the true clusters for each of the four datasets?   \n","(2) For each dataset, explain why K-Means might struggle to correctly recover the true clusters. (Hint: think about the distributions data points for each cluster and how that might impact the Euclidean distance metric that K-Mean uses.)"],"metadata":{"id":"86q-fcYKHDRh"}},{"cell_type":"code","source":["# Enter your code here"],"metadata":{"id":"W_XkZgC7lTfI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Does Data Standardization Fix the Problem?    \n","\n","How can we fix the problems that we observe above? One simple idea would be to standardize our datasets as we've learned to do before. In the code block below, use the Z-score transform to standardize the X variable for each of the four datasets. Then retrain the K-means model for dataset using the normalized X data, predict the clusters, and generate new scatter plots for each dataset showing the predicted clusters.           \n","\n","Discuss with your lab group:  \n","(1) Did data standardization improve the clustering results? Why or why not?"],"metadata":{"id":"nHIufbuBJkgd"}},{"cell_type":"code","source":["# Enter your code here"],"metadata":{"id":"dbRfnUmslbH6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Selecting the Optimal Number of Clusters    \n","\n","Our primary problem for the Gaussian Blob dataset is that we selected the wrong number of clusters. There are clearly three distinct clusters, we told K-Means to only create two clusters. While that discrepancy is very obvious in this simple 2D scenario with highly separable data, it may not be so clear cut for messy data with higher dimensionality.     \n","Recall from lecture that we can use the silhouette score to help us pick the optimal number of clusters for K-Means. Let's test 2, 3, and 4 clusters to see which performs best on our dataset. The code shell below will create silhouette plots for each cluster number choice, as well as print out the average silhouette score. Fill in the blank code lines to instantiate a new K-Means model object for a given number of clusters, train the model on the X data, and then calculate the average silhouette score.    \n","\n","Discuss with your lab group:  \n","(1) Which number of clusters performs the best? What features of the silhouette plot tell you that this is a well-performing model?"],"metadata":{"id":"kyKE3i4EKbhu"}},{"cell_type":"code","source":["range_n_clusters = [2, 3, 4]\n","\n","for n_clusters in range_n_clusters:\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    fig.set_size_inches(10, 4)\n","\n","    ax1.set_xlim([-0.1, 1])\n","    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n","\n","    # Enter code here to instantiate a new K-Means object\n","    clusterer =\n","    # Enter code here to train the K-Means model\n","    cluster_labels =\n","\n","    # Enter code here to calculate the silhouette score\n","    silhouette_avg =\n","    print(\n","        \"For n_clusters =\",\n","        n_clusters,\n","        \"The average silhouette_score is :\",\n","        silhouette_avg,\n","    )\n","\n","    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n","\n","    y_lower = 10\n","    for i in range(n_clusters):\n","        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(\n","            np.arange(y_lower, y_upper),\n","            0,\n","            ith_cluster_silhouette_values,\n","            facecolor=color,\n","            edgecolor=color,\n","            alpha=0.7,\n","        )\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","        y_lower = y_upper + 10\n","\n","    ax1.set_title(\"The silhouette plot for the various clusters.\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    ax1.set_yticks([])\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","    ax2.scatter(\n","        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n","    )\n","\n","    centers = clusterer.cluster_centers_\n","    ax2.scatter(\n","        centers[:, 0],\n","        centers[:, 1],\n","        marker=\"o\",\n","        c=\"white\",\n","        alpha=1,\n","        s=200,\n","        edgecolor=\"k\",\n","    )\n","\n","    for i, c in enumerate(centers):\n","        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n","\n","    ax2.set_title(\"The visualization of the clustered data.\")\n","    ax2.set_xlabel(\"Feature space for the 1st feature\")\n","    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n","\n","    plt.suptitle(\n","        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n","        % n_clusters,\n","        fontsize=14,\n","        fontweight=\"bold\",\n","    )\n","\n","plt.show()"],"metadata":{"id":"EcosUrdbm_F5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dealing with Different Size Clusters   \n","\n","A simple way that we can improve the K-Means model peformance for unevenly size clusters is to re-run the model more times with different randomly generated starting centroids. This increase the likelihood that one of these centroids will be near the cluster with the smaller number of features, ensuring that it is captured in the final clustering results.     \n","For the unevenly size blobs dataset, train a new K-Means model where `n_init=10`, so that the algorithm will try 10 different randomly selected centroid seeds.        \n","\n","Discuss with your lab group:         \n","(1) Did increasing the number of initializations improve the results? How would this scale if you had a very large or high dimensional dataset?"],"metadata":{"id":"hJBLdQYDNbqs"}},{"cell_type":"code","source":["# Enter your code here"],"metadata":{"id":"Pn6PR1E8nCiC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dealing with Anisotropy and Uneven Variance    \n","\n","Unfortunately, the assumptions underlying K-Means mean that this model will never perform particularly well when the true clusters have anisotropic or uneven variances. These are cases where Gaussian Mixture Models (GMMs) may perform better.      \n","\n","In the code block below, for the anisotropically distributed blobs and the unequal variance blobs, instantiate a new Gaussian Mixture Model with 3 components, train it on the appropriate X data, and predict the clusters. Create two scatter plots, one for each datasets, of the X vs. Y data with data points colored by the cluster that the GMM predicts they belong to.      \n","\n","Discuss with your lab group:    \n","(1) Did GMM perform better than K-Means for these datasets? Why might it peform better in these specific cases?  "],"metadata":{"id":"Qn59q5hiOfaU"}},{"cell_type":"code","source":["# Enter your code here"],"metadata":{"id":"lHVxNVl-nGTo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GMM Hyperparameter Tuning  \n","\n","One of our parameter choices for GMMs is the structure of the covariance matrix. Scikit-Learn offers four different options:   \n","\n","`full`: each component has its own general covariance matrix.       \n","`tied`: all components share the same general covariance matrix.        \n","`diag`: each component has its own diagonal covariance matrix.           \n","`spherical`: each component has its own single variance.         \n","\n","The code below demonstrates how to use `GridSearchCV` to simultaneously optimize the number of clusters and the structure of the covariance matrix using the Bayesian Information Criterion (BIC) score. The lower the BIC score, the better the model performance. Fill in the line of code to fit the `gridsearch` objects on the X data from the anisotropically distributed blobs, then run the code block.\n","\n","Discuss with your lab group:            \n","(1) What is the best choice for the number of clusters and covariance matrix structure? Is this consistent with what you can see in the data? Why or why not?\n"],"metadata":{"id":"JcBvjnYMPZCX"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","import seaborn as sns\n","import pandas as pd\n","\n","def gmm_bic_score(estimator, X):\n","    return -estimator.bic(X)\n","\n","\n","param_grid = {\n","    \"n_components\": range(1, 7),\n","    \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n","}\n","grid_search = GridSearchCV(\n","    GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score\n",")\n","# Enter your code to train the grid search object on the X data here\n","\n","\n","df = pd.DataFrame(grid_search.cv_results_)[\n","    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n","]\n","df[\"mean_test_score\"] = -df[\"mean_test_score\"]\n","df = df.rename(\n","    columns={\n","        \"param_n_components\": \"Number of components\",\n","        \"param_covariance_type\": \"Type of covariance\",\n","        \"mean_test_score\": \"BIC score\",\n","    }\n",")\n","df.sort_values(by=\"BIC score\").head()\n","\n","sns.catplot(\n","    data=df,\n","    kind=\"bar\",\n","    x=\"Number of components\",\n","    y=\"BIC score\",\n","    hue=\"Type of covariance\",\n",")\n","plt.show()"],"metadata":{"id":"G3wiX3PaplJQ"},"execution_count":null,"outputs":[]}]}