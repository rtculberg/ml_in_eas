{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO8CSr1PQOjZ"
   },
   "source": [
    "# Problem Set 2 - Principal Component Analysis\n",
    "\n",
    "The main objective of this problem set is for you to practice implementing principal component analysis in Scikit-Learn on a real geophysical dataset.    \n",
    "\n",
    "### Scientific Premise:   \n",
    "Ice slabs are multi-meter thick layers of refrozen ice that form in snow and firn on the Greenland Ice Sheet. They lead to more ice sheet mass loss and sea level rise by preventing snow from absorbing meltwater and enhancing meltwater runoff. Therefore, we are very interested in understanding where these ice slabs are located and how fast they are growing. Unfortunately, they are challenging to map since they are buried beneath the surface and we only have limited data about their extent from some airborne radar flight lines. Over the next two problem sets, we will work on predicting the location of ice slabs using remote sensing and climate model data. In this problem set, you will start by preparing the dataset for machine learning.   \n",
    "\n",
    "### Data Set Variables:    \n",
    "**X** - x coordinate of data point (EPSG 3413)       \n",
    "**Y** - y coordinate of data point (EPSG 3413)         \n",
    "**IceSlab** - binary variable indicating if an ice slab was observed at a given location, 0 = no ice slab, 1 = ice slab observed            \n",
    "**HV** - horizonal-vertical polarized backscatter from the Sentinel-1 SAR satellite in digital number space        \n",
    "**SAR_melt** - non-dimensional estimate of firn/snow water content, lower values indicate more stored melt           \n",
    "**MAR** - decadal average melt to accumulation (snowfall) ratio from a regional climate model         \n",
    "**RACMO_melt** - decadal average surface melt from the regional climate model RACMO in mm of water equivalent per year           \n",
    "**RACMO_snow** - decadal average snowfall rate from the regional climate model RACMO in mm of water equivalent per year          \n",
    "**MAT** - mean annual temperature in degrees Celcius from the regional climate model MAR           \n",
    "**Xpol** - cross polarized backscater ratio from the Sentinel-1 SAR satellite in digital number space             \n",
    "**Elevation** - surface elevation in meters           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z-7NP_eSjhC"
   },
   "source": [
    "In the code block below, import your packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRJmiCsm8Fi0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eyzz7pvxSrBS"
   },
   "source": [
    "**[1] (5 pts)** Load the from the course github at https://raw.githubusercontent.com/rtculberg/ml_in_eas/main/data/IceSlabs.csv. Display the first few rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PPn-ZTM8QBW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61V9yRNTS2oh"
   },
   "source": [
    "**[2] (5 pts)** Check for and remove any rows that have NaN values. Remove the X and Y columns since we are not going to usual spatial position variables when we eventually train our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgLUDnmY8eou"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFieXwhJTGJT"
   },
   "source": [
    "**[3] (5 pts)** Make a copy of the dataframe and drop the \"IceSlab\" column. Normalize the data using a z-score transform. Display the first few rows of your newly normalized dataframe and print out the standard deviation of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWzQtz028zkC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbLWfz9fUBER"
   },
   "source": [
    "**[4] (5 pts)** Compute the correlation matrix for the normalized data and make a colormap plot of these values using the pandas background_gradient function.     \n",
    "\n",
    "Answer the following question:          \n",
    "Would you expect any problems if we tried to apply our machine learning algorithm directly to this dataset before PCA? Why or why not?\n",
    "\n",
    "**Hint**: See the styler documentation: https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.background_gradient.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xStkNHcK9Jw4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc9Py2nEUkK9"
   },
   "source": [
    "**[5] (10 pts)** Apply a PCA and determine the principal components (PCs) using the sklearn PCA fit_transform function. Leave the n_components argument blank. This will default to keeping all components.\n",
    "\n",
    "**Hint**: See the `PCA`: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSBg1j0X9fjA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpqP0_laUpLb"
   },
   "source": [
    "**[6] (5 pts)** Make a scatter plot of PC1 vs. PC2. Use a different color for points where ice slabs were observed vs. points where iceslabs were not observed. Make sure to include a legend. Describe any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXca8pl09qkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCOvn8RbU1sD"
   },
   "source": [
    "**[7] (5 pts)** Make a scree plot showing the percent variance explained by each principle component.       \n",
    "\n",
    "Answer the following question:                \n",
    "How many principal components do we need to retain to explain at least 90% of the variance? How does that compare to the original number of raw variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO78kbDf-q8D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aN3mOsOGVP6C"
   },
   "source": [
    "**[8] (5 pts)** Use the function provided below to make a biplot from the principal components and loadings.      \n",
    "\n",
    "Answer the following question:            \n",
    "Which of the original variables are mostly strongly associated with PC1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF2_31vQMA03"
   },
   "outputs": [],
   "source": [
    "# this function will plot a biplot given principal components, loadings, and variable labels\n",
    "\n",
    "def biplot(PCs,coef,labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    xs = PCs[:,0] # PC1 (change indices for different PCs)\n",
    "    ys = PCs[:,1] # PC2\n",
    "    coef = np.transpose(coef)\n",
    "    n = coef.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley,\n",
    "                s=15,\n",
    "                color='red')\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coef[i,0],\n",
    "                  coef[i,1],color = 'purple',\n",
    "                  alpha = 0.5)\n",
    "        plt.text(coef[i,0]* 1.15,\n",
    "                 coef[i,1] * 1.15,\n",
    "                 labels[i],\n",
    "                 color = 'darkblue',\n",
    "                 ha = 'center',\n",
    "                 va = 'center')\n",
    "\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title('Biplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf4Tb8h_MDeW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-nCm4CgVt50"
   },
   "source": [
    "**[9] (5 pts)** Use `imshow` to display the loadings matrix. Be sure to correctly label the axes and use a diverging colorbar between -1 and 1.      \n",
    "\n",
    "Answer the questions below:      \n",
    "Describe how the loading matrix might have looked different if we did not have such strong correlations between most of the raw data variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jx-QcfYaNbbX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPoyUp0Nw7O/FVui0hKKndF",
   "provenance": [
    {
     "file_id": "1yTkI91LbLOnc_TDESQ-1dmR7tey3a9E4",
     "timestamp": 1722951219506
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
